{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f230416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=pd.errors.DtypeWarning)\n",
    "\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import urllib.parse\n",
    "import base64\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b01f38",
   "metadata": {},
   "source": [
    "# Data Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3fc1b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"C:\\\\Users\\\\Pratt\\\\Desktop\\\\HKUST-RA\\\\[-Ongoing-] Text analysis in Glassdoor data\\\\Basic Data\\\\glassdoor-ep-revirew\"\n",
    "csv_files = []\n",
    "df_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c264c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ëé∑ÂèñÊâÄÊúâCSVÊñá‰ª∂\n",
    "for item in os.listdir(folder_path):\n",
    "    if item.endswith('.csv'):\n",
    "        full_path = os.path.join(folder_path, item)\n",
    "        csv_files.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3593c79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä ËØªÂèñCSVÊñá‰ª∂: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 76/76 [01:57<00:00,  1.55s/Êñá‰ª∂]\n"
     ]
    }
   ],
   "source": [
    "success_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "# ‰ΩøÁî®ÊôÆÈÄöÂæ™ÁéØÔºå‰∏ç‰ΩøÁî®low_memory=False\n",
    "for file_path in tqdm(csv_files, desc=\"üìä ËØªÂèñCSVÊñá‰ª∂\", unit=\"Êñá‰ª∂\"):\n",
    "    try:\n",
    "        # ‰∏ç‰ΩøÁî®low_memory=FalseÔºå‰ΩøÁî®ÈªòËÆ§ËÆæÁΩÆ\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_list.append(df)\n",
    "        success_count += 1\n",
    "    except Exception as e:\n",
    "        failed_count += 1\n",
    "        # ‰∏ç‰∏≠Êñ≠tqdmÔºåÂè™ËÆ∞ÂΩïÈîôËØØ\n",
    "        tqdm.write(f\"‚ùå ËØªÂèñÂ§±Ë¥•: {os.path.basename(file_path)} - {str(e)[:50]}...\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4dbb493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2507e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_column = \"ÂÖ¨Âè∏ID\"  # ÊõøÊç¢‰∏∫‰Ω†ÁöÑÂàóÂêç\n",
    "new_df = (df_full[df_full.groupby(group_column)[group_column].transform('size') >= 100]\n",
    "          .groupby(group_column)\n",
    "          .first()\n",
    "          .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e064bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_1 = new_df['ÂÖ¨Âè∏Âêç'].tolist()\n",
    "df_list_2 = new_df['ÂÖ¨Âè∏ÁΩëÂùÄ'].tolist()\n",
    "df_list_3 = new_df['ÂÖ¨Âè∏ÁÆÄ‰ªã'].tolist()\n",
    "df = pd.DataFrame({'company': df_list_1, 'url': df_list_2, 'intro': df_list_3})\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "381edd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully found URL for AAR\n",
      "Successfully found URL for AMETEK\n",
      "Successfully found URL for American Airlines\n",
      "Successfully found URL for Abbott\n",
      "Successfully found URL for BankUnited\n",
      "Successfully found URL for AMD\n",
      "Successfully found URL for Aetna\n",
      "Successfully found URL for Air Products\n",
      "Successfully found URL for Airgas\n",
      "Successfully found URL for Alaska Airlines\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    max_retries = 5\n",
    "    retry_count = 0\n",
    "    found_url = None\n",
    "    company = df.loc[i, 'company']\n",
    "    url = df.loc[i, 'url']\n",
    "    intro = df.loc[i, 'intro']\n",
    "    \n",
    "    while retry_count < max_retries and not found_url:\n",
    "        options = uc.ChromeOptions()\n",
    "\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--disable-infobars')\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "\n",
    "        driver = uc.Chrome(options=options)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # ------------------------------------------------------------------------------ # BingÊêúÁ¥¢\n",
    "        try:           \n",
    "            driver.get(\"https://www.bing.com/\")\n",
    "            time.sleep(2.5)\n",
    "            \n",
    "            # ÊâæÂà∞ÊêúÁ¥¢Ê°ÜÂÖÉÁ¥†Âπ∂ËæìÂÖ•Êü•ËØ¢\n",
    "            search_box = driver.find_element(By.NAME, \"q\")\n",
    "            search_query = f'{company} glassdoor {intro[:10]}'\n",
    "            search_box.clear()\n",
    "            search_box.send_keys(search_query)\n",
    "            time.sleep(1.5)\n",
    "            \n",
    "            # Êèê‰∫§ÊêúÁ¥¢ÔºàÊåâÂõûËΩ¶ÈîÆÔºâ\n",
    "            search_box.send_keys(Keys.RETURN)\n",
    "            time.sleep(4)\n",
    "\n",
    "            result_links = driver.find_elements(By.CSS_SELECTOR, \"h2 a\")\n",
    "            time.sleep(1)\n",
    "            current_found_url = None\n",
    "\n",
    "            for i, link in enumerate(result_links, 1):\n",
    "                decoded_url = link.get_attribute(\"href\")\n",
    "                parsed_url = urlparse(decoded_url)\n",
    "                # Ê£ÄÊü•ÊòØÂê¶ÂåÖÂê´bing.com\n",
    "                if \"www.bing.com\" in decoded_url:\n",
    "                    u_param = parse_qs(parsed_url.query).get('u', [None])[0]\n",
    "                    if not u_param:\n",
    "                        continue\n",
    "                    try:\n",
    "                        b64_str = u_param[2:]\n",
    "                        padding = len(b64_str) % 4\n",
    "                        if padding:\n",
    "                            b64_str += \"=\" * (4 - padding)\n",
    "                        decoded_bytes = base64.urlsafe_b64decode(b64_str)\n",
    "                        decoded_url = decoded_bytes.decode('utf-8')\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                try:\n",
    "                    # Á¨¨‰∏ÄÊ≠•ÔºöÁ≠õÈÄâglassdoorÈìæÊé•\n",
    "                    if \"glassdoor\" not in decoded_url:\n",
    "                        continue\n",
    "                    # Á¨¨‰∫åÊ≠•ÔºöÂÖàÁ≠õÈÄâOverviewÈìæÊé•\n",
    "                    if \"/Overview/\" in decoded_url:\n",
    "                        current_found_url = decoded_url\n",
    "                        break\n",
    "                    # Á¨¨‰∏âÊ≠•ÔºöÂ¶ÇÊûúÊ≤°ÊúâOverviewÔºåÁ≠õÈÄâReviewsÈìæÊé•\n",
    "                    elif \"/Reviews/\" in decoded_url:\n",
    "                        current_found_url = decoded_url\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            if current_found_url:\n",
    "                found_url = current_found_url\n",
    "                break\n",
    "                \n",
    "        except Exception as e:  \n",
    "            print(f\"Error processing {company} on retry {retry_count + 1}: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            # Á°Æ‰øùÊØèÊ¨°Â∞ùËØïÂêéÈÉΩÂÖ≥Èó≠driver\n",
    "            driver.quit()\n",
    "            time.sleep(1)\n",
    "        \n",
    "        retry_count += 1\n",
    "        \n",
    "        # Â¶ÇÊûú‰∏çÊòØÊúÄÂêé‰∏ÄÊ¨°ÈáçËØïÔºåÁ≠âÂæÖ‰∏Ä‰ºöÂÑøÂÜçÈáçËØï\n",
    "        if retry_count < max_retries and not found_url:\n",
    "            time.sleep(2)  # ÈáçËØïÂâçÁ≠âÂæÖÊó∂Èó¥\n",
    "    \n",
    "    # Ê†πÊçÆÊúÄÁªàÁªìÊûúÂÜ≥ÂÆöÊ∑ªÂä†‰ªÄ‰πà\n",
    "    if found_url:\n",
    "        results.append({'company': company, 'url': found_url})\n",
    "        print(f\"Successfully found URL for {company}\")\n",
    "    else:\n",
    "        results.append({'company': company, 'url': None})\n",
    "        print(f\"Failed to find valid Glassdoor link for {company} after {max_retries} attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f180be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‰øùÂ≠ò‰∏∫ JSON Êñá‰ª∂\n",
    "with open('C:\\\\Users\\\\Pratt\\\\Desktop\\\\HKUST-RA\\\\[-Ongoing-] Text analysis in Glassdoor data\\\\Url-Scrap-Method-2-Glassdoor-ep\\\\Urls-2.1.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
